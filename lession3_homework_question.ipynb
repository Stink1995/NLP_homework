{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What's the model? why all the models are wrong, but some are useful? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model是经过训练以识别特定类型的模式的文件\n",
    "# 所有的模型的是错误的，但是有些事有用地，科学并不一定是正确的，只是在一定时间内和一定条件下的相对正确， \n",
    "# 而在将来的某天，可能被完善，甚至被彻底颠覆，科学的进步就是不断地用新方法去验证前人的思想，否定前人的\n",
    "# 观念同时提出符合当前时代背景的新观念，因此科学具备可证伪性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underfitting : 欠拟合, 通俗讲就是模型不能很好的拟合数据，即表现出比较高的偏差\n",
    "# underfitting的原因 : 选择的模型不合理;数据的特征较少\n",
    "# overfitting :  过拟合, 模型具有比较高的方差，在训练集上能够做到很好甚至完美的拟合数据，但是不具有泛化性，当引入新数据时，拟合情况不是很好\n",
    "# overfitting的原因 :  数据量太少;样本数据中存在着噪音;参数太多,模型复杂度太高;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision : 精确率 表示预测出为正的样本中有多少是正确地 precision = (TP) / (TP + FP)\n",
    "# recall : 召回率    表示实际为正的样本中预测出了多少正确地正样本 recall = (TP) / (TP + FN)\n",
    "# AUC : ROC曲线下的面积 自己设定一个阈值,一般为0.5,当ROC大于阈值时，则可以判定为正，小于阈值判定为负\n",
    "#     ROC 曲线 : 横坐标是 FPR , 纵坐标是 TPR \n",
    "# F1 : F值是精确率和召回率的调和值, 2/F1 = 1/precision + 1/recall\n",
    "# F2 : F2 = (1+β**2)/(β**2)(precission*recall / (precision + recall)) 当β==1时为F1score,当β>1时为F2score\n",
    "\n",
    "# precison,recall,AUC,F1,F2score 这些评估方法主要用于评估分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Based on our course and yourself mind, what's the machine learning? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机器学习与传统的分析式编程最主要的区别在于分析的角色不同，传统编程通常是程序员设定好算法，然后通过编写if-else等代码语句来分析已达到期望\n",
    "# 输出的结果，而机器学习则是让机器自己学习分析，而不是按照这程序员设定的代码一步一步执行操作，可以根据本身学习出来的模型对于未知的情况进行\n",
    "# 预测，而传统编程则是人为的分析好，然后转换成代码逻辑，进行预测时需要修改参数，本质上还是一个人为的过程。随着科技的进步，人类生活产生越来\n",
    "# 越多的数据，海量的数据单靠人为来分析已经很难满足当前的需求了，而且未来数据量只会越来越大，这样就需要充分利用高速计算机的性能，让计算机自\n",
    "# 己掌握学习的能力，处理越来越多的分析需求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我觉得这句话有一定道理,机器学习模型的评价标准应该是符合业务需求地，根据不同的业务需求评价标准也会不同,不单单只是机器学习问题任何工作中的\n",
    "# 问题都应该先从明确业务需求开始，只有明确了需求才能设计出有价值的机器学习模型,所以在设计模型之前,应该先选好要用什么指标来评价模型,比如分\n",
    "# 类问题那么我们就要选用 AUC 或者 F-score,回归问题则选用MAE,MSE,聚类问题选用rand index,Mutual Information,如果不能使用正确的模型\n",
    "# 评估指标那么就很难得到好的模型，机器学习模型是服务于业务地，在不同的业务场景下要求不同,例如:在做良次品分类时，可以设定不同的阈值，根据业\n",
    "# 务需求设定合适的阈值才能产生最好的结果。社会不断进步地，一个模型建立好了以后也不可能一成不变，为了适应时代的发展，就需要不断地对模型进行\n",
    "# 调优，那么在调优过程中模型的评价标准就是一个很重要的选择，能够帮助我们不断地完善我们的模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
