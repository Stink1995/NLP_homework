{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import jieba\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Based AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_rules = '''\n",
    "say_hello = names hello tail\n",
    "names = name names | name \n",
    "name = stink | mai | 老李\n",
    "hello = hello | 早上好 | 你好 \n",
    "tail = 呀 | !\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_by_gram(grammar_str:str,target,stmt_split='=',expr_split='|'):\n",
    "    rules = dict()\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line:continue\n",
    "        stmt,expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(expr_split)\n",
    "    \n",
    "    generated = generate(rules,target = target)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammer_rules,target):\n",
    "    if target in grammer_rules:\n",
    "        candidates = grammer_rules[target]\n",
    "        candidate = random.choice(candidates)\n",
    "        return ''.join(generate(grammer_rules,target = c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老李mai早上好!\n",
      "stink老李mai老李maimai早上好呀\n",
      "老李老李你好!\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(make_sentence_by_gram(hello_rules,'say_hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => Adj | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个小小的桌子听着一个蓝色的桌子\n",
      "这个小小的蓝色的好看的桌子坐在这个小小的小猫\n",
      "这个蓝色的小小的桌子看着这个好看的小小的蓝色的小猫\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(make_sentence_by_gram(simple_grammar,target = 'sentence',stmt_split = '=>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model  \n",
    "## 2-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(sentence)  = Pr( w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i,w_{i+1})} {count(w_i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '/Users/liyehong/Desktop/nlp/lession1/article_9k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = open(text_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33425826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外自本周6月12日起除小米手机6等15款机型外其余机型已暂停更新发布含开发版体验版内测稳定版暂不受影响以确保工程师可以集中全部精力进行系统优化工作有人猜测这也是将精力主要用到MIUI9的研发之中MIUI8去年5月发布距今已有一年有余也是时候更新换代了当然关于MIUI9的确切信息我们还是等待官方消息\\n骁龙835作为唯一通过Windows10桌面平台认证的ARM处理器高通强调不会因为只考虑性能而去屏蔽掉小核心相反他们正联手微软找到一种适合桌面平台的兼顾性能和功耗的完美方案报道称微软已经拿到了一些新的源码以便Windows10更好地理解biglittle架构资料显示骁龙835作为一款集成了CPUGPU基带蓝牙WiFi的SoC比传统的Wintel方案可以节省至少30的PCB空间按计划今年Q4华硕惠普联想将首发骁龙835Win10电脑预计均是二合一形态的产品当然高通骁龙只是个开始未来也许还能见到三星Exynos联发科华为麒麟小米澎湃等进入Windows10桌面平台\\n此前的一加3T搭载的是3400mAh电池DashCharge快充规格为5V4A至于电池缩水可能与刘作虎所说一加手机5要做市面最轻薄大屏旗舰的设定有关按照目前掌握的资料一加手机5拥有55寸1080P三星AMOLED显示屏6G8GBRAM64GB128GBROM双1600万摄像头备货量惊喜根据京东泄露的信息一加5起售价是xx99元应该是在279928992999中的某个\\n这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车新华社记者张立云摄\\n原标题44岁女子跑深圳约会网友被拒暴雨中裸身奔走深圳交警微博称昨日清晨交警发现有一女子赤裸上身行走在南坪快速上期间还起了轻生年头一辅警发现后赶紧为其披上黄衣并一路劝说她那么事发时到底都发生了些什么呢南都记者带您一起还原现场南都记者在龙岗大队坂田中队见到了辅警刘青发现女生的辅警一位外表高大帅气说话略带些腼腆的90后青年刘青介绍6月16日早上7时36分他正在环城南路附近值勤接到中队关于一位女子裸身进入机动车可能有危险的警情随后骑着小铁骑开始沿路寻找大概花了十多分钟在南坪大道坂田出口往龙岗方向的逆行辅道上发现该女子女子身上一丝不挂地逆车流而行时走时停时坐时躺险象环生刘青停好小铁骑和另外一名巡防员追了上去发现女子的情绪很低落话不多刘青尝试和女子交流劝说女子离开可女子并不愿意'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100000\n",
    "sub_file = FILE[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(file):\n",
    "    return list(jieba.cut(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3x/p6tf9w8s1z325p6pg0rzf59w0000gn/T/jieba.cache\n",
      "Loading model cost 0.904 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "TOKENS = cut(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17618254"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 703716),\n",
       " ('n', 382020),\n",
       " ('在', 263597),\n",
       " ('月', 189330),\n",
       " ('日', 166300),\n",
       " ('新华社', 142462),\n",
       " ('和', 134061),\n",
       " ('年', 123106),\n",
       " ('了', 121938),\n",
       " ('是', 100909),\n",
       " ('\\n', 89611),\n",
       " ('１', 88187),\n",
       " ('０', 84945),\n",
       " ('外代', 83268),\n",
       " ('中', 73926),\n",
       " ('中国', 71179),\n",
       " ('２', 70521),\n",
       " ('2017', 69894),\n",
       " ('记者', 62147),\n",
       " ('二线', 61998)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_with_fre = [f for w , f in words_count.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(words_with_fre[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.log(np.log(words_with_fre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(jieba.cut('人的梦想是永远不会完结地'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i + 1] for i in range(len(TOKENS) - 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外自', '自本周', '本周6', '6月', '月12', '12日起', '日起除', '除小米', '小米手机', '手机6']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_words_count = Counter(two_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n新华社', 69033),\n",
       " ('2017年', 61480),\n",
       " ('外代二线', 61301),\n",
       " ('n新华社', 59794),\n",
       " ('日n', 52216),\n",
       " ('新华社照片', 50401),\n",
       " ('5月', 37977),\n",
       " ('4月', 34571),\n",
       " ('新华社记者', 30864),\n",
       " ('２０', 27166),\n",
       " ('日在', 27154),\n",
       " ('年5', 25433),\n",
       " ('n当日', 25241),\n",
       " ('年4', 23727),\n",
       " ('n外代', 20854),\n",
       " ('照片外代', 20777),\n",
       " ('比赛中', 20637),\n",
       " ('外代2017', 20463),\n",
       " ('n5月', 20426),\n",
       " ('n4月', 19920)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_gram_count(word):\n",
    "    if word in words_count:\n",
    "        return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(get_one_gram_count('自然语言'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_gram_count(word):\n",
    "    if word in two_gram_words_count:\n",
    "        return two_gram_words_count[word]\n",
    "    else:\n",
    "        return two_gram_words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(get_two_gram_count('怎么说'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word,wc):\n",
    "    if word in wc:\n",
    "        return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gram_count('你是谁',two_gram_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    \n",
    "    probility = 1\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        two_gram_c = get_gram_count(word + next_word,two_gram_words_count)\n",
    "        one_gran_c = get_gram_count(word,words_count)\n",
    "        pro = two_gram_c / one_gran_c\n",
    "        \n",
    "        probility *= pro\n",
    "    \n",
    "    return probility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('白石麻衣天下第一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4111065629690267e-05"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('第一白石麻衣天下')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.918959982837208e-16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('人的梦想是永远不会完结地')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.25106196201993e-17"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('人的是永远不会梦想完结地')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project-1 设计自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 场景：与朋友打电话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "human = subject verb noun tail\n",
    "subject = 你 | 兄弟 | 铁汁 | 老铁 | bro\n",
    "verb = 在 | 想 | 有\n",
    "noun = 吃饭 | 打游戏 | 看电影 | 学习 \n",
    "tail = 吗 | ？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'兄弟有看电影吗'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentence_by_gram(human,target = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(grammar_str:str,target,n=1,stmt_split='=',expr_split='|'):\n",
    "    for i in range(n):\n",
    "        sentence = make_sentence_by_gram(grammar_str,target,stmt_split='=',expr_split='|')\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兄弟有吃饭？\n",
      "兄弟有打游戏吗\n",
      "兄弟有学习？\n",
      "你想打游戏？\n",
      "bro想学习吗\n",
      "你有吃饭吗\n",
      "你在吃饭吗\n",
      "老铁在看电影？\n",
      "你在看电影吗\n",
      "铁汁想看电影吗\n"
     ]
    }
   ],
   "source": [
    "generate_n(human,target='human',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend = \"\"\"\n",
    "friend = hello separate name separate answer separate sorry separate end tail\n",
    "hello = hi | hello | hey\n",
    "name = stink | my bro | 铁汁 | 老李\n",
    "separate = , | ! | .\n",
    "answer = 我要 reason \n",
    "reason = 帮妈妈做家务 | 照顾弟弟 | 去外婆家 | 外出旅行 | \n",
    "sorry = 抱歉 | 对不起 | 不好意思\n",
    "end = 下次约 | 忙完联系你\n",
    "tail = !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!铁汁.我要去外婆家!对不起!忙完联系你!!!\n",
      "hey.铁汁!我要帮妈妈做家务.不好意思,忙完联系你!!!\n",
      "hi!老李.我要外出旅行.对不起,下次约!!!\n",
      "hey,mybro,我要照顾弟弟,抱歉.忙完联系你!!!\n",
      "hi!stink,我要去外婆家!不好意思,下次约!!!\n"
     ]
    }
   ],
   "source": [
    "generate_n(friend,target='friend',n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project-2 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = '/Users/liyehong/Desktop/nlp/lession1/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(train_txt_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1343520"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ++$++ Is  Disability  Insurance  Required  By  Law?\\n1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++$++ Can  Creditors  Take  Life  Insurance  After  Death?\\n2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ ++$++ Does  Travelers  Insurance  Have  Renters  Insurance?\\n3 ++$++ auto-insurance ++$++ 我可以开一辆没有保险的新车吗？ ++$++ Can  I  Drive  A  New  Car  Home  Without  Insurance?\\n4 ++$++ life-insurance ++$++ 人寿保险的现金转出价值是否应纳税？ ++$++ Is  The  Cash  Surrender  Value  Of  Life  Insurance  Taxable?\\n5 ++$++ annuities ++$++ 如何报告年金收入？ ++$++ How  Is  Annuity  Income  Reported?\\n6 ++$++ home-insurance ++$++ AAA家庭保险涵盖什么？ ++$++ What  Does  AAA  Home  Insurance  Cover?\\n7 ++$++ retirement-plans ++$++ 什么是简单的退休计划？ ++$++ What  Is  A  Simple  Retirement  Plan?\\n8 ++$++ disability-insurance ++$++ 社会保险残疾保险是什么？ ++$++ What  Does  Social  Security  Disability  Insurance  Cover?\\n9 ++$++ auto-insurance ++$++ 汽车保险是否预付？ ++$++ Is  Car  Insurance  Prepaid?\\n10 ++$++ medicare-insurance ++$+'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = re.compile(r'[\\u4e00-\\u9fa5]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = real_text.findall(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ''.join(line for line in train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = cut(ss) # one-gram 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74259"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_count = Counter(train_tokens) # 统计one-gram分词中各个词在文本中出现的频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('保险', 5005),\n",
       " ('的', 3220),\n",
       " ('人寿保险', 2962),\n",
       " ('什么', 2675),\n",
       " ('吗', 2479),\n",
       " ('是', 2344),\n",
       " ('我', 2053),\n",
       " ('是否', 1862),\n",
       " ('可以', 1704),\n",
       " ('健康', 1513),\n",
       " ('如何', 1294),\n",
       " ('医疗保险', 1269),\n",
       " ('多少', 1244),\n",
       " ('汽车保险', 1189),\n",
       " ('在', 913),\n",
       " ('覆盖', 847),\n",
       " ('你', 827),\n",
       " ('有', 770),\n",
       " ('残疾', 724),\n",
       " ('房主', 714)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_two = [\n",
    "    train_tokens[i] + train_tokens[i+1] for i in range(len(train_tokens) - 1)\n",
    "]                     # two-gram 分词               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_count_two = Counter(train_tokens_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('健康保险', 1347),\n",
       " ('什么是', 1152),\n",
       " ('保险是否', 975),\n",
       " ('我的', 726),\n",
       " ('残疾保险', 658),\n",
       " ('房主保险', 602),\n",
       " ('我可以', 530),\n",
       " ('保险吗', 511),\n",
       " ('是否覆盖', 504),\n",
       " ('家庭保险', 440),\n",
       " ('年金', 427),\n",
       " ('长期护理', 416),\n",
       " ('是什么', 380),\n",
       " ('护理保险', 367),\n",
       " ('的人寿保险', 319),\n",
       " ('吗什么', 317),\n",
       " ('人寿保险吗', 312),\n",
       " ('你可以', 299),\n",
       " ('退休计划', 289),\n",
       " ('保险的', 280)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_count_two.most_common(20)  # 统计two-gram分词中各个词在文本中出现的频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word,wc):\n",
    "    if word in wc:\n",
    "        return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insurance_two_gram_model(sentence):\n",
    "    sentence_tokens = cut(sentence)\n",
    "    \n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(sentence_tokens) - 1):\n",
    "        word = sentence_tokens[i]\n",
    "        next_word = sentence_tokens[i+1]\n",
    "        \n",
    "        one_gram_count = get_gram_count(word,train_words_count)\n",
    "        two_gram_count = get_gram_count(next_word,train_words_count_two)\n",
    "        pro = two_gram_count / one_gram_count\n",
    "        \n",
    "        probability *= pro\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.516879125604471e-07"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('汽车保险是否预付')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4666691333358e-07"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('汽车预付保险是否')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6223816978004468e-10"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('法律要求残疾保险吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.06363736835441e-11"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('法律是否大家可好看要求残疾保险')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2655444669740255e-06"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('我今天早上没有吃早饭')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5003605741549836e-08"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_two_gram_model('我今天早上没有吃晚饭和早饭')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## project-3 获取最优质的的语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rules = \"\"\"\n",
    "sentence_rules = name date staus place event\n",
    "name = stink | mai | 寅子 | lex | 老李\n",
    "date = year month day \n",
    "year = 2019年 | 2018年 | 2017年\n",
    "month = 1月 | 2月 | 3月 | 4月 | 5月 | 6月 | 7月 | 8月 | 9月 | 10月 | 11月 | 12月 | 13月 | 14月\n",
    "day = 1日 | 2日 | 3日 | 4日 | 5日 | 6日 | 7日 | 31日 | 32日 | 33日 |\n",
    "staus = 在 | 去 | 回 | 到 | 爬 | 跑 | 走\n",
    "place = 上海 | 北京 | 深圳 | 长沙\n",
    "event = 出差 | 吃早饭 | 买房 | 下棋 | 打游戏 | 旅行 | 游泳 | 上学 | 谈恋爱 | 唱歌 | 回家 | 战狼 | 洗澡 | 自然语言处理\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stink2017年6月到深圳吃早饭\n",
      "lex2017年9月31日爬北京买房\n",
      "lex2018年2月7日回北京买房\n",
      "寅子2019年4月5日在长沙买房\n",
      "lex2017年13月7日在深圳战狼\n",
      "stink2017年1月回北京打游戏\n",
      "老李2019年14月2日走北京出差\n",
      "stink2019年13月5日走长沙洗澡\n",
      "老李2017年4月7日在上海买房\n",
      "寅子2018年7月33日到深圳回家\n"
     ]
    }
   ],
   "source": [
    "generate_n(sentence_rules,target='sentence_rules',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_list(grammar_str:str,target,n=1,stmt_split='=',expr_split='|'):\n",
    "    sentence_list = []\n",
    "    for i in range(n):\n",
    "        sentence = make_sentence_by_gram(grammar_str,target,stmt_split='=',expr_split='|')\n",
    "        sentence_list.append(sentence)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_l = generate_n_list(sentence_rules,target='sentence_rules',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['老李2019年12月6日爬深圳打游戏', 'mai2018年10月32日在长沙自然语言处理', '老李2017年14月7日在上海回家', 'stink2017年4月2日走深圳游泳', '寅子2017年12月4日在上海吃早饭', '老李2017年4月5日去上海上学', '老李2019年8月6日在深圳打游戏', '寅子2019年11月4日到上海战狼', 'stink2017年9月4日爬北京下棋', 'mai2018年8月2日回深圳谈恋爱']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    \n",
    "    probility = 1\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        two_gram_c = get_gram_count(word + next_word,two_gram_words_count)\n",
    "        one_gran_c = get_gram_count(word,words_count)\n",
    "        pro = two_gram_c / one_gran_c\n",
    "        \n",
    "        probility *= pro\n",
    "    \n",
    "    return probility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(sentence_rules,target,n=1,stmt_split='=',expr_split='|'):\n",
    "    sentence_l = generate_n_list(sentence_rules,target,n,stmt_split,expr_split)\n",
    "    \n",
    "    sentence_probility_list = []\n",
    "    \n",
    "    for s in sentence_l:\n",
    "        probility = two_gram_model(s)\n",
    "        t = (s , probility)\n",
    "#         print(t)\n",
    "        sentence_probility_list.append(t)\n",
    "#     print(sentence_probility_list)\n",
    "    new_list = sorted(sentence_probility_list , key = lambda x : x[1] , reverse=True)\n",
    "#     print(new_list)\n",
    "    return new_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stink2019年6月5日爬上海出差', 1.928318247799569e-16)\n",
      "('stink2018年6月2日在上海唱歌', 1.3353989616287027e-11)\n",
      "('老李2018年14月2日走北京战狼', 1.5591373088454192e-21)\n",
      "('lex2018年14月31日爬北京回家', 1.6533598219244346e-20)\n",
      "('stink2019年12月1日去深圳洗澡', 4.917837787406215e-13)\n",
      "('mai2017年10月31日跑北京打游戏', 3.3245354428453914e-18)\n",
      "('老李2019年6月7日回长沙打游戏', 4.8311730606388784e-11)\n",
      "('寅子2017年13月2日在长沙唱歌', 1.449909156538374e-16)\n",
      "('mai2017年11月6日走深圳下棋', 1.7690381773611766e-17)\n",
      "('mai2019年6月31日在北京吃早饭', 1.9293890939568776e-13)\n",
      "[('老李2019年6月7日回长沙打游戏', 4.8311730606388784e-11), ('stink2018年6月2日在上海唱歌', 1.3353989616287027e-11), ('stink2019年12月1日去深圳洗澡', 4.917837787406215e-13), ('mai2019年6月31日在北京吃早饭', 1.9293890939568776e-13), ('stink2019年6月5日爬上海出差', 1.928318247799569e-16), ('寅子2017年13月2日在长沙唱歌', 1.449909156538374e-16), ('mai2017年11月6日走深圳下棋', 1.7690381773611766e-17), ('mai2017年10月31日跑北京打游戏', 3.3245354428453914e-18), ('lex2018年14月31日爬北京回家', 1.6533598219244346e-20), ('老李2018年14月2日走北京战狼', 1.5591373088454192e-21)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('老李2019年6月7日回长沙打游戏', 4.8311730606388784e-11)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(sentence_rules,target='sentence_rules',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
