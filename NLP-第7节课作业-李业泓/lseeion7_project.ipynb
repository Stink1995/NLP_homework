{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 利用通配符打印出所有的文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Czech.txt', 'data/names/German.txt', 'data/names/Arabic.txt', 'data/names/Japanese.txt', 'data/names/Chinese.txt', 'data/names/Vietnamese.txt', 'data/names/Russian.txt', 'data/names/French.txt', 'data/names/Irish.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Greek.txt', 'data/names/Italian.txt', 'data/names/Portuguese.txt', 'data/names/Scottish.txt', 'data/names/Dutch.txt', 'data/names/Korean.txt', 'data/names/Polish.txt']\n"
     ]
    }
   ],
   "source": [
    "# glob.glob()返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径\n",
    "def find_files(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "print(find_files('data/names/*.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 名字中包含不少非Ascii码，将他们转换成Ascii码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string.ascii_letters 生成a-zA-Z所有的字母\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_2_Ascii(s):\n",
    "    '''\n",
    "    unicodedata.normalize('NFD',s) 表示字符s应该分解成多个组合字符表示 例如:Ś 分解成 S 和 上标符号\n",
    "    unicodedata.category(s) 表示字符s在unicode里面的分类类型 Mn:标记 非间距\n",
    "    '''\n",
    "    ret = ''.join([c for c in unicodedata.normalize('NFD',s) \n",
    "                   if unicodedata.category(c) != 'Mn' and c in all_letters])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.建立类别与对应的名字的dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = {}\n",
    "all_categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    lines = open(filename,encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_2_Ascii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "os.path.basename(path) 返回path最后的文件名 如 path最后以 \\ 或者 / 结尾 则返回空\n",
    "os.path.splitext(filename) 分离文件名与扩展名 如 输入china.txt 返回 china 和 .txt\n",
    "'''\n",
    "for filename in find_files('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''总共有多少个类别的数量'''\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.使用pytorch进行向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''查找字符在all_letters中的索引'''\n",
    "def letter_to_index(letter):\n",
    "    return all_letters.find(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_tensor(letter):\n",
    "    '''将字符letter向量化'''\n",
    "    tensor = torch.zeros(1,n_letters)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line):\n",
    "    '''将字符串line向量化'''\n",
    "    tensor = torch.zeros(len(line),1,n_letters)\n",
    "    for li,letter in enumerate(line):\n",
    "        tensor[li][0][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.构建RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,inputs,hidden):\n",
    "        \n",
    "        combined = torch.cat((inputs,hidden),1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs维度\n",
    "n_letters\n",
    "# hidden维度\n",
    "n_hidden = 128\n",
    "# output维度\n",
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.运行RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = letter_to_tensor('L')\n",
    "hidden = torch.zeros(1,n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对于对象rnn直接在后面加()传参数相当于调用了RNN类中的forward方法,因为在RNN的父类 nn.Module中定义了__call__()方法\n",
    "'''\n",
    "output,next_hidden = rnn(inputs,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.将output变成人们方便识别的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    # topk(n)返回最大的n个数据 top_n 为 value top_i 为 index\n",
    "    top_n,top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return all_categories[category_i],category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Chinese', 4)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_from_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.随机生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(l):\n",
    "    return l[random.randint(0,len(l)-1)]\n",
    "\n",
    "def get_fact_sample(category):\n",
    "    facts = category_lines[category]\n",
    "    fact_sample = sample(facts)\n",
    "    return fact_sample\n",
    "\n",
    "def sample_trainning():\n",
    "    category = sample(all_categories)\n",
    "    line = get_fact_sample(category)\n",
    "    category_tensor = torch.tensor([all_categories.index(category)],dtype=torch.long)\n",
    "    line_tensor = line_to_tensor(line)\n",
    "    return category,line,category_tensor,line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category= Portuguese / line= Torres\n",
      "category= Arabic / line= Salib\n",
      "category= Czech / line= Alt\n",
      "category= Czech / line= Klemper\n",
      "category= Spanish / line= Barros\n",
      "category= Scottish / line= Lindsay\n",
      "category= Spanish / line= Petit\n",
      "category= Spanish / line= Gutierrez\n",
      "category= Russian / line= Astrakhankin\n",
      "category= Korean / line= Bang\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category,line,category_tensor,line_tensor = sample_trainning()\n",
    "    print('category=',category,'/ line=',line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.使用交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train(category_tensor,line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output , hidden = rnn(line_tensor[i] , hidden)\n",
    "        \n",
    "    loss = criterion(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 50% (0m 1s) 2.1553 Koulaxizis / Greek ✓\n",
      "1000 100% (0m 2s) 2.8088 Hakimi / Japanese ✗ (Arabic)\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    seconds = now - since\n",
    "    minute = math.floor(seconds / 60)\n",
    "    seconds -= minute * 60\n",
    "    \n",
    "    return '%dm %ds' % (minute,seconds)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    output,loss = train(category_tensor,line_tensor)\n",
    "    current_loss += loss\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess , guess_i = category_from_output(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss, line, guess, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.观察Loss的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7297172284126283,\n",
       " 2.7527003359794615,\n",
       " 2.7185774564743044,\n",
       " 2.738234317302704,\n",
       " 2.707736620903015,\n",
       " 2.7611072516441344,\n",
       " 2.758996150493622,\n",
       " 2.6786904203891755,\n",
       " 2.7377470314502714,\n",
       " 2.6890264534950257]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output,hidden = rnn(line_tensor[i],hidden)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line,n_prediction=3):\n",
    "    print('\\n> %s'% input_line)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = evaluate(line_to_tensor(input_line))\n",
    "        \n",
    "        topv,topi = output.topk(n_prediction,1,True)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(n_prediction):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value,all_categories[category_index]))\n",
    "            predictions.append([value,all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Mai\n",
      "(-2.63) Italian\n",
      "(-2.64) Chinese\n",
      "(-2.68) Vietnamese\n",
      "\n",
      "> Stink\n",
      "(-2.71) Polish\n",
      "(-2.74) English\n",
      "(-2.74) Japanese\n",
      "\n",
      "> Yuki\n",
      "(-2.52) Japanese\n",
      "(-2.60) Polish\n",
      "(-2.60) Italian\n"
     ]
    }
   ],
   "source": [
    "predict('Mai')\n",
    "predict('Stink')\n",
    "predict('Yuki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 尝试在我们的RNN模型中添加更多layers，然后观察Loss变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_V1(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size1,hidden_size2,output_size):\n",
    "        super(RNN_V1,self).__init__()\n",
    "        \n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        \n",
    "        self.i2h1 = nn.Linear(input_size + hidden_size1 , hidden_size1)\n",
    "        self.i2h2 = nn.Linear(hidden_size1 + hidden_size2 , hidden_size2)\n",
    "        self.i2o = nn.Linear(hidden_size1 + hidden_size2 , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs,hidden1,hidden2):\n",
    "        \n",
    "        combined1 = torch.cat((inputs,hidden1),1)\n",
    "        hidden1 = self.i2h1(combined1)\n",
    "        output1 = self.relu(hidden1)\n",
    "        combined2 = torch.cat((output1,hidden2),1)\n",
    "        hidden2 = self.i2h2(combined2)\n",
    "        output = self.i2o(combined2)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output,hidden2\n",
    "    \n",
    "    def initHidden(self,hidden_size1,hidden_size2):\n",
    "        hidden1 = torch.zeros(1,hidden_size1)\n",
    "        hidden2 = torch.zeros(1,hidden_size2)\n",
    "        return hidden1 , hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs维度\n",
    "n_letters\n",
    "# hidden1 维度\n",
    "n_hidden1 = 128\n",
    "#hidden2 维度\n",
    "n_hidden2 = 64\n",
    "# output维度\n",
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_v1 = RNN_V1(n_letters,n_hidden1,n_hidden2,n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_V1(\n",
       "  (i2h1): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2h2): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (i2o): Linear(in_features=192, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train_v1(category_tensor,line_tensor):\n",
    "    hidden1,hidden2 = rnn_v1.initHidden(n_hidden1,n_hidden2)\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    rnn_v1.zero_grad()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output ,hidden2 = rnn_v1(line_tensor[i] , hidden1,hidden2)\n",
    "        \n",
    "    loss = criterion(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn_v1.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***现在正在输出多layers RNN的结果***\n",
      "500 25% (0m 2s) 3.0019 Rutten / Spanish ✗ (Dutch)\n",
      "***现在正在输出单layers RNN的结果***\n",
      "500 25% (0m 2s) 2.7401 Rutten / German ✗ (Dutch)\n",
      "***现在正在输出多layers RNN的结果***\n",
      "1000 50% (0m 5s) 2.9124 Luo / Scottish ✗ (Chinese)\n",
      "***现在正在输出单layers RNN的结果***\n",
      "1000 50% (0m 5s) 2.5574 Luo / Korean ✗ (Chinese)\n",
      "***现在正在输出多layers RNN的结果***\n",
      "1500 75% (0m 7s) 2.8061 Corti / Italian ✓\n",
      "***现在正在输出单layers RNN的结果***\n",
      "1500 75% (0m 7s) 2.4139 Corti / Italian ✓\n",
      "***现在正在输出多layers RNN的结果***\n",
      "2000 100% (0m 10s) 2.8150 Sokal / Italian ✗ (Polish)\n",
      "***现在正在输出单layers RNN的结果***\n",
      "2000 100% (0m 10s) 2.9171 Sokal / Arabic ✗ (Polish)\n"
     ]
    }
   ],
   "source": [
    "n_iters = 2000\n",
    "\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "\n",
    "# 多layers RNN\n",
    "current_loss1 = 0\n",
    "all_losses1 = []\n",
    "# 单leyers RNN\n",
    "current_loss2 = 0\n",
    "all_losses2 = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    # 多layers RNN\n",
    "    output1,loss1 = train_v1(category_tensor,line_tensor)\n",
    "    current_loss1 += loss1\n",
    "    # 单layers RNN\n",
    "    output2,loss2 = train(category_tensor,line_tensor)\n",
    "    current_loss2 += loss2\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess1 , guess_i1 = category_from_output(output1)\n",
    "        correct = '✓' if guess1 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出多layers RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss1, line, guess1, correct))\n",
    "        \n",
    "    if iters % print_every == 0 :\n",
    "        guess2 , guess_i2 = category_from_output(output2)\n",
    "        correct = '✓' if guess2 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出单layers RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss2, line, guess2, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses1.append(current_loss1 / plot_every)\n",
    "        all_losses2.append(current_loss2 / plot_every)\n",
    "        current_loss1 = 0\n",
    "        current_loss2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多layers RNN:\n",
      "[2.885951325893402, 2.8919425296783445, 2.880792078971863, 2.8943641281127928, 2.887309763431549, 2.8920133018493654, 2.8980507373809816, 2.8875694108009338, 2.8883215618133544, 2.882408928871155, 2.8792552161216736, 2.8868811440467836, 2.8839226508140565, 2.8816251826286314, 2.873859176635742, 2.871556842327118, 2.8744288897514343, 2.8726945543289184, 2.877608821392059, 2.8697048830986023]\n",
      "单layers RNN:\n",
      "[2.7213080859184267, 2.6502015471458433, 2.720110001564026, 2.671740838289261, 2.629564433097839, 2.662118192911148, 2.7003481471538544, 2.5614106261730196, 2.6042903447151184, 2.5923542284965517, 2.5200770235061647, 2.5601474404335023, 2.5654919707775115, 2.5232647252082825, 2.5174214839935303, 2.4091359400749206, 2.503758072257042, 2.385744194984436, 2.5450039571523666, 2.451318130493164]\n"
     ]
    }
   ],
   "source": [
    "print('多layers RNN:')\n",
    "print(all_losses1)\n",
    "print('单layers RNN:')\n",
    "print(all_losses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'疑问  为什么多layers RNN loss反而比单layers loss 大呢？\\n我检查了好久 没发现问题 希望老师在改作业后在评语里指点一下  谢谢啦'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''疑问  为什么多layers RNN loss反而比单layers loss 大呢？\n",
    "我检查了好久 没发现问题 希望老师在改作业后在评语里指点一下  谢谢啦'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 将原始的RNN模型改成nn.LSTM和nn.GRU， 并且改变 n_iters = 1000 这个值，观察其变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,inputs,hidden,control):\n",
    "        \n",
    "        combined = torch.cat((inputs,hidden),1)\n",
    "        a = self.i2h(combined)\n",
    "        \n",
    "        z = self.tanh(a)\n",
    "        zi = self.sigmod(a)\n",
    "        zf = self.sigmod(a)\n",
    "        zo = self.sigmod(a)\n",
    "        \n",
    "        control = zf.mul(control) + zi.mul(z)\n",
    "        hidden = zo.mul(self.tanh(control))\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output,hidden,control\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(n_letters,n_hidden,n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=128, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       "  (tanh): Tanh()\n",
       "  (sigmod): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train_lstm(category_tensor,line_tensor):\n",
    "    hidden = lstm.initHidden()\n",
    "    control = lstm.initHidden()\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    lstm.zero_grad()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output , hidden ,control = lstm(line_tensor[i] , hidden,control)\n",
    "        \n",
    "    loss = criterion(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in lstm.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***现在正在输出LSTM的结果***\n",
      "500 50% (0m 2s) 2.9531 Zhang / Japanese ✗ (Chinese)\n",
      "***现在正在输出RNN的结果***\n",
      "500 50% (0m 2s) 0.9672 Zhang / Chinese ✓\n",
      "***现在正在输出LSTM的结果***\n",
      "1000 100% (0m 5s) 2.8784 Pantelas / German ✗ (Greek)\n",
      "***现在正在输出RNN的结果***\n",
      "1000 100% (0m 5s) 0.8430 Pantelas / Greek ✓\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "# LSTM\n",
    "current_loss1 = 0   \n",
    "all_losses1 = []\n",
    "# RNN\n",
    "current_loss2 = 0\n",
    "all_losses2 = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    # LSTM\n",
    "    output1,loss1 = train_lstm(category_tensor,line_tensor)\n",
    "    current_loss1 += loss1\n",
    "    # RNN\n",
    "    output2,loss2 = train(category_tensor,line_tensor)\n",
    "    current_loss2 += loss2\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess1 , guess_i1 = category_from_output(output1)\n",
    "        correct = '✓' if guess1 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出LSTM的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss1, line, guess1, correct))\n",
    "        \n",
    "    if iters % print_every == 0 :\n",
    "        guess2 , guess_i2 = category_from_output(output2)\n",
    "        correct = '✓' if guess2 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss2, line, guess2, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses1.append(current_loss1 / plot_every)\n",
    "        all_losses2.append(current_loss2 / plot_every)\n",
    "        current_loss1 = 0\n",
    "        current_loss2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8746918559074404,\n",
       " 2.874691483974457,\n",
       " 2.869926104545593,\n",
       " 2.888092164993286,\n",
       " 2.875089776515961,\n",
       " 2.871559782028198,\n",
       " 2.877913990020752,\n",
       " 2.866274833679199,\n",
       " 2.873765397071838,\n",
       " 2.869029121398926,\n",
       " 2.868867003917694,\n",
       " 2.8674361085891724,\n",
       " 2.875820209980011,\n",
       " 2.8756304502487184,\n",
       " 2.8702869272232054,\n",
       " 2.8669332814216615,\n",
       " 2.866185784339905,\n",
       " 2.878901560306549,\n",
       " 2.866950137615204,\n",
       " 2.8700921154022216,\n",
       " 2.8705575942993162,\n",
       " 2.862789945602417,\n",
       " 2.864740924835205,\n",
       " 2.874198396205902,\n",
       " 2.8708564949035646,\n",
       " 2.858320565223694,\n",
       " 2.8755458307266237,\n",
       " 2.8719739937782287,\n",
       " 2.872731921672821,\n",
       " 2.86985143661499,\n",
       " 2.8627507948875426,\n",
       " 2.85831524848938,\n",
       " 2.8577174878120424,\n",
       " 2.8526731610298155,\n",
       " 2.863429026603699,\n",
       " 2.8750904273986815,\n",
       " 2.8583113312721253,\n",
       " 2.855660719871521,\n",
       " 2.852945771217346,\n",
       " 2.8661801195144654,\n",
       " 2.846739752292633,\n",
       " 2.8481362676620483,\n",
       " 2.869725930690765,\n",
       " 2.8535204410552977,\n",
       " 2.8539528274536132,\n",
       " 2.862327363491058,\n",
       " 2.8580382871627807,\n",
       " 2.849465093612671,\n",
       " 2.858138976097107,\n",
       " 2.842781250476837]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Loss\n",
    "all_losses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.368366609811783,\n",
       " 2.3800265204906466,\n",
       " 2.26980160176754,\n",
       " 2.1485344287753105,\n",
       " 2.310079976320267,\n",
       " 2.309486210346222,\n",
       " 2.198187758922577,\n",
       " 2.223685482889414,\n",
       " 2.2997890892624855,\n",
       " 2.1631899851560594,\n",
       " 2.274751192331314,\n",
       " 2.213223315179348,\n",
       " 2.301973067522049,\n",
       " 2.2916450840234757,\n",
       " 2.2561871933937074,\n",
       " 2.312960294485092,\n",
       " 2.2485823914408685,\n",
       " 2.017235503345728,\n",
       " 2.3089068818092344,\n",
       " 2.23375236004591,\n",
       " 2.3194860780239104,\n",
       " 2.3144896566867827,\n",
       " 2.116139702796936,\n",
       " 2.1691104716062544,\n",
       " 2.0746486197412013,\n",
       " 2.1496936348080635,\n",
       " 2.1594113618135453,\n",
       " 2.213214085102081,\n",
       " 2.28453542560339,\n",
       " 1.98828871935606,\n",
       " 2.1903842318058016,\n",
       " 2.1506259302794932,\n",
       " 2.2134738764166833,\n",
       " 2.0398297011852264,\n",
       " 2.1299920573830606,\n",
       " 2.2089452774822713,\n",
       " 2.0084380255639553,\n",
       " 2.104708690345287,\n",
       " 2.0012562365829947,\n",
       " 2.184931677877903,\n",
       " 2.1167761573195456,\n",
       " 2.0497565034031866,\n",
       " 2.1721402701735495,\n",
       " 2.0642763060331344,\n",
       " 2.035502164512873,\n",
       " 1.983254341483116,\n",
       " 2.1099649310112,\n",
       " 2.0824568292498586,\n",
       " 2.2400856107473373,\n",
       " 2.025098505616188]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN Loss\n",
    "all_losses2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(GRU,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,inputs,hidden):\n",
    "        \n",
    "        combined = torch.cat((inputs,hidden),1)\n",
    "        a = self.i2h(combined)\n",
    "        \n",
    "        z = self.sigmod(a)\n",
    "        r = self.sigmod(a)\n",
    "        \n",
    "        hidden_1 = hidden.mul(r)\n",
    "        combined2 = torch.cat((inputs,hidden_1),1)\n",
    "        a1 = self.i2h(combined2)\n",
    "        h = self.tanh(a1)\n",
    "        \n",
    "        hidden = z.mul(hidden) + (1 - z).mul(h)\n",
    "        \n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(n_letters,n_hidden,n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train_gru(category_tensor,line_tensor):\n",
    "    hidden = gru.initHidden()\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    gru.zero_grad()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output , hidden = gru(line_tensor[i] , hidden)\n",
    "        \n",
    "    loss = criterion(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in gru.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***现在正在输出GRU的结果***\n",
      "500 10% (0m 3s) 2.8469 Mata / Spanish ✗ (Portuguese)\n",
      "***现在正在输出RNN的结果***\n",
      "500 10% (0m 3s) 3.0739 Mata / Japanese ✗ (Portuguese)\n",
      "***现在正在输出GRU的结果***\n",
      "1000 20% (0m 7s) 2.8579 Tochikura / Portuguese ✗ (Japanese)\n",
      "***现在正在输出RNN的结果***\n",
      "1000 20% (0m 7s) 0.2611 Tochikura / Japanese ✓\n",
      "***现在正在输出GRU的结果***\n",
      "1500 30% (0m 10s) 2.7759 Rios / Chinese ✗ (Portuguese)\n",
      "***现在正在输出RNN的结果***\n",
      "1500 30% (0m 10s) 2.4378 Rios / Greek ✗ (Portuguese)\n",
      "***现在正在输出GRU的结果***\n",
      "2000 40% (0m 14s) 2.8074 Nieves / Portuguese ✗ (Spanish)\n",
      "***现在正在输出RNN的结果***\n",
      "2000 40% (0m 14s) 2.2065 Nieves / Portuguese ✗ (Spanish)\n",
      "***现在正在输出GRU的结果***\n",
      "2500 50% (0m 17s) 2.8728 Donoghue / English ✗ (Irish)\n",
      "***现在正在输出RNN的结果***\n",
      "2500 50% (0m 17s) 2.6515 Donoghue / French ✗ (Irish)\n",
      "***现在正在输出GRU的结果***\n",
      "3000 60% (0m 21s) 2.8435 Tsumemasa / Spanish ✗ (Japanese)\n",
      "***现在正在输出RNN的结果***\n",
      "3000 60% (0m 21s) 0.5246 Tsumemasa / Japanese ✓\n",
      "***现在正在输出GRU的结果***\n",
      "3500 70% (0m 24s) 2.7480 Riagan / Irish ✓\n",
      "***现在正在输出RNN的结果***\n",
      "3500 70% (0m 24s) 1.1044 Riagan / Irish ✓\n",
      "***现在正在输出GRU的结果***\n",
      "4000 80% (0m 28s) 2.9295 Hiorvst / Irish ✗ (Czech)\n",
      "***现在正在输出RNN的结果***\n",
      "4000 80% (0m 28s) 2.3622 Hiorvst / French ✗ (Czech)\n",
      "***现在正在输出GRU的结果***\n",
      "4500 90% (0m 32s) 2.9103 Seto / Irish ✗ (Chinese)\n",
      "***现在正在输出RNN的结果***\n",
      "4500 90% (0m 32s) 1.6983 Seto / Korean ✗ (Chinese)\n",
      "***现在正在输出GRU的结果***\n",
      "5000 100% (0m 35s) 2.7709 Rhys / Greek ✗ (Irish)\n",
      "***现在正在输出RNN的结果***\n",
      "5000 100% (0m 35s) 3.3199 Rhys / Korean ✗ (Irish)\n"
     ]
    }
   ],
   "source": [
    "n_iters = 5000\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "# GRU\n",
    "current_loss1 = 0   \n",
    "all_losses1 = []\n",
    "# RNN\n",
    "current_loss2 = 0\n",
    "all_losses2 = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    # GRU\n",
    "    output1,loss1 = train_gru(category_tensor,line_tensor)\n",
    "    current_loss1 += loss1\n",
    "    # RNN\n",
    "    output2,loss2 = train(category_tensor,line_tensor)\n",
    "    current_loss2 += loss2\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess1 , guess_i1 = category_from_output(output1)\n",
    "        correct = '✓' if guess1 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出GRU的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss1, line, guess1, correct))\n",
    "        \n",
    "    if iters % print_every == 0 :\n",
    "        guess2 , guess_i2 = category_from_output(output2)\n",
    "        correct = '✓' if guess2 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss2, line, guess2, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses1.append(current_loss1 / plot_every)\n",
    "        all_losses2.append(current_loss2 / plot_every)\n",
    "        current_loss1 = 0\n",
    "        current_loss2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****GRU*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.893720610141754,\n",
       " 2.8847940802574157,\n",
       " 2.885782995223999,\n",
       " 2.882409162521362,\n",
       " 2.881149389743805,\n",
       " 2.8807849621772768,\n",
       " 2.878885102272034,\n",
       " 2.8676226806640623,\n",
       " 2.860492997169495,\n",
       " 2.880382194519043,\n",
       " 2.883038890361786,\n",
       " 2.876764600276947,\n",
       " 2.8755289483070374,\n",
       " 2.896203107833862,\n",
       " 2.8749061107635496,\n",
       " 2.8844662857055665,\n",
       " 2.876378688812256,\n",
       " 2.8725864577293394,\n",
       " 2.8686391282081605,\n",
       " 2.871350691318512,\n",
       " 2.8767373538017273,\n",
       " 2.874485261440277,\n",
       " 2.8593267512321474,\n",
       " 2.874884297847748,\n",
       " 2.8663968300819396,\n",
       " 2.870559823513031,\n",
       " 2.8720271468162535,\n",
       " 2.859490547180176,\n",
       " 2.8692965006828306,\n",
       " 2.8452204775810244,\n",
       " 2.87095388174057,\n",
       " 2.8631205654144285,\n",
       " 2.876673855781555,\n",
       " 2.8731401324272157,\n",
       " 2.8457133436203,\n",
       " 2.858735284805298,\n",
       " 2.859727690219879,\n",
       " 2.86003940820694,\n",
       " 2.8603555965423584,\n",
       " 2.8496090888977053,\n",
       " 2.8372186279296874,\n",
       " 2.853773331642151,\n",
       " 2.8574480605125427,\n",
       " 2.8557692885398867,\n",
       " 2.841560192108154,\n",
       " 2.859740414619446,\n",
       " 2.8326065039634702,\n",
       " 2.8455194354057314,\n",
       " 2.828581783771515,\n",
       " 2.851155707836151]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"*****GRU*****\")\n",
    "all_losses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****RNN*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6506175993010401,\n",
       " 1.90040491938591,\n",
       " 1.784052850306034,\n",
       " 1.797691224887967,\n",
       " 1.8138391511887313,\n",
       " 1.7869735908508302,\n",
       " 1.9064816419035195,\n",
       " 1.8194462862610816,\n",
       " 1.8746739821881055,\n",
       " 1.8324728465080262,\n",
       " 1.7360979357361794,\n",
       " 1.5592568418383599,\n",
       " 1.8262093536555768,\n",
       " 1.8589673418551684,\n",
       " 1.850891173928976,\n",
       " 1.6741281850636005,\n",
       " 1.878282853513956,\n",
       " 1.7375369933247566,\n",
       " 1.7985009685903788,\n",
       " 1.7369938434660435,\n",
       " 1.6666024950146676,\n",
       " 1.9613409201800822,\n",
       " 1.861621048077941,\n",
       " 1.8835010581463576,\n",
       " 1.7810922824963926,\n",
       " 1.6397405238449574,\n",
       " 1.933578432686627,\n",
       " 1.9376843455433845,\n",
       " 1.7132708989828824,\n",
       " 1.5891053189337254,\n",
       " 1.6950543866679073,\n",
       " 1.6917134291678666,\n",
       " 1.8855361287295818,\n",
       " 1.5863108824193477,\n",
       " 1.6864173837564886,\n",
       " 1.6311909140273928,\n",
       " 1.7016473835706711,\n",
       " 1.7971281175315381,\n",
       " 1.8035540939867496,\n",
       " 1.6947978019528092,\n",
       " 1.6223999182879925,\n",
       " 2.0280141976475714,\n",
       " 1.7371028871834278,\n",
       " 1.6882592931389808,\n",
       " 1.6783495077490806,\n",
       " 1.789519346728921,\n",
       " 1.8175171756744384,\n",
       " 1.6227541592158377,\n",
       " 1.640842616367154,\n",
       " 1.7694643175415694]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"*****RNN*****\")\n",
    "all_losses2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 把该RNN模型变成多层RNN模型，观察Loss的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN2,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size , input_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size , output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,inputs,hidden):\n",
    "        \n",
    "        combined = torch.cat((inputs,hidden),1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN2(n_letters,n_letters,n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN2(\n",
       "  (i2h): Linear(in_features=114, out_features=57, bias=True)\n",
       "  (i2o): Linear(in_features=114, out_features=18, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train2(category_tensor,line_tensor):\n",
    "    hidden1 = rnn2.initHidden()\n",
    "    hidden2 = rnn2.initHidden()\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    rnn2.zero_grad()\n",
    "    \n",
    "    sencond_inputs = []\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output , hidden1 = rnn2(line_tensor[i] , hidden1)\n",
    "        sencond_inputs.append(hidden1)\n",
    "    for h in range(len(sencond_inputs)):\n",
    "        output , hidden2 = rnn2(sencond_inputs[h],hidden2)\n",
    "        \n",
    "    loss = criterion(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn2.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***现在正在输出多层RNN的结果***\n",
      "500 10% (0m 2s) 2.2804 Yakhnenko / Russian ✓\n",
      "***现在正在输出单层RNN的结果***\n",
      "500 10% (0m 2s) 0.6690 Yakhnenko / Russian ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "1000 20% (0m 5s) 2.8682 Gutierrez / Czech ✗ (Spanish)\n",
      "***现在正在输出单层RNN的结果***\n",
      "1000 20% (0m 5s) 1.2153 Gutierrez / Spanish ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "1500 30% (0m 7s) 2.5020 Kasimor / Greek ✗ (Czech)\n",
      "***现在正在输出单层RNN的结果***\n",
      "1500 30% (0m 7s) 2.2821 Kasimor / Arabic ✗ (Czech)\n",
      "***现在正在输出多层RNN的结果***\n",
      "2000 40% (0m 10s) 2.2565 Attard / Japanese ✗ (English)\n",
      "***现在正在输出单层RNN的结果***\n",
      "2000 40% (0m 10s) 1.9437 Attard / French ✗ (English)\n",
      "***现在正在输出多层RNN的结果***\n",
      "2500 50% (0m 12s) 2.7598 Freitas / Greek ✗ (Portuguese)\n",
      "***现在正在输出单层RNN的结果***\n",
      "2500 50% (0m 12s) 1.2936 Freitas / Portuguese ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "3000 60% (0m 15s) 1.6951 Awad / Arabic ✓\n",
      "***现在正在输出单层RNN的结果***\n",
      "3000 60% (0m 15s) 1.2004 Awad / Arabic ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "3500 70% (0m 17s) 1.4490 Shuo / Korean ✗ (Chinese)\n",
      "***现在正在输出单层RNN的结果***\n",
      "3500 70% (0m 17s) 1.6235 Shuo / Korean ✗ (Chinese)\n",
      "***现在正在输出多层RNN的结果***\n",
      "4000 80% (0m 20s) 0.7527 Warszawski / Polish ✓\n",
      "***现在正在输出单层RNN的结果***\n",
      "4000 80% (0m 20s) 0.3579 Warszawski / Polish ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "4500 90% (0m 22s) 2.1312 Schrijnemakers / Greek ✗ (Dutch)\n",
      "***现在正在输出单层RNN的结果***\n",
      "4500 90% (0m 22s) 0.5040 Schrijnemakers / Dutch ✓\n",
      "***现在正在输出多层RNN的结果***\n",
      "5000 100% (0m 25s) 0.5873 Sotiris / Greek ✓\n",
      "***现在正在输出单层RNN的结果***\n",
      "5000 100% (0m 25s) 0.4938 Sotiris / Greek ✓\n"
     ]
    }
   ],
   "source": [
    "n_iters = 5000\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "# 多层RNN\n",
    "current_loss1 = 0   \n",
    "all_losses1 = []\n",
    "# 单层RNN\n",
    "current_loss2 = 0\n",
    "all_losses2 = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    # 多层RNN\n",
    "    output1,loss1 = train2(category_tensor,line_tensor)\n",
    "    current_loss1 += loss1\n",
    "    # 单层RNN\n",
    "    output2,loss2 = train(category_tensor,line_tensor)\n",
    "    current_loss2 += loss2\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess1 , guess_i1 = category_from_output(output1)\n",
    "        correct = '✓' if guess1 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出多层RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss1, line, guess1, correct))\n",
    "        \n",
    "    if iters % print_every == 0 :\n",
    "        guess2 , guess_i2 = category_from_output(output2)\n",
    "        correct = '✓' if guess2 == category else '✗ (%s)' % category\n",
    "        print('***现在正在输出单层RNN的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss2, line, guess2, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses1.append(current_loss1 / plot_every)\n",
    "        all_losses2.append(current_loss2 / plot_every)\n",
    "        current_loss1 = 0\n",
    "        current_loss2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****多层RNN*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6914182925224304,\n",
       " 2.6663350439071656,\n",
       " 2.629610325098038,\n",
       " 2.6034675860404968,\n",
       " 2.545738945007324,\n",
       " 2.5613265240192415,\n",
       " 2.4636453652381896,\n",
       " 2.4468847790360453,\n",
       " 2.4790514588356016,\n",
       " 2.391013212800026,\n",
       " 2.4410859644412994,\n",
       " 2.4378731799125672,\n",
       " 2.3516276919841768,\n",
       " 2.35990096449852,\n",
       " 2.355634834468365,\n",
       " 2.32068509221077,\n",
       " 2.363510847091675,\n",
       " 2.230501587241888,\n",
       " 2.2534689250588418,\n",
       " 2.229762721657753,\n",
       " 2.275754176080227,\n",
       " 2.129536775946617,\n",
       " 2.194841649532318,\n",
       " 2.183096173405647,\n",
       " 2.314688563644886,\n",
       " 2.0682940724492074,\n",
       " 2.1791704535484313,\n",
       " 2.002256373241544,\n",
       " 2.1270493584871293,\n",
       " 2.3663136833906173,\n",
       " 2.125438638627529,\n",
       " 2.070333033800125,\n",
       " 2.1414993649721143,\n",
       " 2.0872655564546587,\n",
       " 2.1606117691099644,\n",
       " 2.1327601355314254,\n",
       " 2.224880239367485,\n",
       " 2.111905972510576,\n",
       " 2.1590923546254635,\n",
       " 2.123886049389839,\n",
       " 2.1612658050656317,\n",
       " 2.257351344525814,\n",
       " 1.972505216896534,\n",
       " 2.1200771646201613,\n",
       " 2.049088230133057,\n",
       " 2.118472330570221,\n",
       " 2.1095726814866067,\n",
       " 2.1327398672699927,\n",
       " 2.00404365375638,\n",
       " 1.9798775520920753]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"*****多层RNN*****\")\n",
    "all_losses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****单层RNN*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.720785503089428,\n",
       " 1.5889805012056604,\n",
       " 1.5779910457506776,\n",
       " 1.4663517324067652,\n",
       " 1.5641002970188855,\n",
       " 1.6835120202600955,\n",
       " 1.6494250504672527,\n",
       " 1.6245457633212208,\n",
       " 1.6242474947869778,\n",
       " 1.5332370697706939,\n",
       " 1.5669024156592786,\n",
       " 1.7303452488780022,\n",
       " 1.492285533361137,\n",
       " 1.5786627519130707,\n",
       " 1.6716720640659333,\n",
       " 1.5110215187445284,\n",
       " 1.6863150171376764,\n",
       " 1.4880665805190803,\n",
       " 1.599388481726637,\n",
       " 1.5342576113343238,\n",
       " 1.740769415833056,\n",
       " 1.337001416236162,\n",
       " 1.7211052253842354,\n",
       " 1.6127065877616404,\n",
       " 1.658262789696455,\n",
       " 1.6163846035674214,\n",
       " 1.606808689981699,\n",
       " 1.5299408080708234,\n",
       " 1.4790775046730413,\n",
       " 1.9366396598517894,\n",
       " 1.570744334757328,\n",
       " 1.5212892780080438,\n",
       " 1.7288183481246233,\n",
       " 1.5592882024496795,\n",
       " 1.5434037662670017,\n",
       " 1.511865881551057,\n",
       " 1.675853467658162,\n",
       " 1.523870654590428,\n",
       " 1.5942396591417491,\n",
       " 1.6292838803865015,\n",
       " 1.6915586812794208,\n",
       " 1.8894137078523636,\n",
       " 1.4519678528048099,\n",
       " 1.5857489686831832,\n",
       " 1.5374054498970509,\n",
       " 1.620724435225129,\n",
       " 1.5933929305151104,\n",
       " 1.4983191972598433,\n",
       " 1.5051400966010988,\n",
       " 1.623793920725584]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"*****单层RNN*****\")\n",
    "all_losses2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pytorch里边常用nn.NLLoss来代替crossentropy，将criterion改为nn.NLLoss，观察变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion4 = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train_v4(category_tensor,line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    # 将rnn中所有模型的参数梯度设置为0\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output , hidden = rnn(line_tensor[i] , hidden)\n",
    "        \n",
    "    loss = criterion4(output , category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        # add_表示张量的相加 以下相当于 -learing_rate * p.grad.data + p.data\n",
    "        p.data.add_(-learning_rate,p.grad.data)\n",
    "        \n",
    "    return output , loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***使用crossentropy损失函数的结果***\n",
      "500 25% (0m 2s) 1.4325 Kunisada / Japanese ✓\n",
      "***使用NLLLoss的结果***\n",
      "500 25% (0m 2s) 1.3258 Kunisada / Japanese ✓\n",
      "***使用crossentropy损失函数的结果***\n",
      "1000 50% (0m 4s) 1.2978 Pho / Vietnamese ✓\n",
      "***使用NLLLoss的结果***\n",
      "1000 50% (0m 4s) 1.2657 Pho / Vietnamese ✓\n",
      "***使用crossentropy损失函数的结果***\n",
      "1500 75% (0m 5s) 1.5661 Gorecki / Polish ✓\n",
      "***使用NLLLoss的结果***\n",
      "1500 75% (0m 5s) 1.4245 Gorecki / Polish ✓\n",
      "***使用crossentropy损失函数的结果***\n",
      "2000 100% (0m 8s) 1.3200 Saller / German ✓\n",
      "***使用NLLLoss的结果***\n",
      "2000 100% (0m 8s) 1.2177 Saller / German ✓\n"
     ]
    }
   ],
   "source": [
    "n_iters = 2000\n",
    "\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "\n",
    "# crossentropy\n",
    "current_loss1 = 0\n",
    "all_losses1 = []\n",
    "# NLLLoss\n",
    "current_loss2 = 0\n",
    "all_losses2 = []\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    seconds = now - since\n",
    "    minute = math.floor(seconds / 60)\n",
    "    seconds -= minute * 60\n",
    "    \n",
    "    return '%dm %ds' % (minute,seconds)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iters in range(1,n_iters + 1):\n",
    "    category , line , category_tensor , line_tensor = sample_trainning()\n",
    "    # crossentropy\n",
    "    output1,loss1 = train(category_tensor,line_tensor)\n",
    "    current_loss1 += loss1\n",
    "    # NLLLoss\n",
    "    output2,loss2 = train_v4(category_tensor,line_tensor)\n",
    "    current_loss2 += loss2\n",
    "    \n",
    "    if iters % print_every == 0 :\n",
    "        guess1 , guess_i1 = category_from_output(output1)\n",
    "        correct = '✓' if guess1 == category else '✗ (%s)' % category\n",
    "        print('***使用crossentropy损失函数的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss1, line, guess1, correct))\n",
    "        \n",
    "    if iters % print_every == 0 :\n",
    "        guess2 , guess_i2 = category_from_output(output2)\n",
    "        correct = '✓' if guess2 == category else '✗ (%s)' % category\n",
    "        print('***使用NLLLoss的结果***')\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iters, iters / n_iters * 100, time_since(start), loss2, line, guess2, correct))\n",
    "        \n",
    "    if iters % plot_every == 0:\n",
    "        all_losses1.append(current_loss1 / plot_every)\n",
    "        all_losses2.append(current_loss2 / plot_every)\n",
    "        current_loss1 = 0\n",
    "        current_loss2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossentropy:\n",
      "[2.445645269751549, 2.532129064798355, 2.4525664973258974, 2.4334159362316132, 2.326144289970398, 2.3374868005514147, 2.383486284613609, 2.306963896751404, 2.3138293850421907, 2.329093291759491, 2.2931398260593414, 2.3485636255145073, 2.3733429205417633, 2.175671352148056, 2.3958961790800095, 2.1925224813818933, 2.1804625084996223, 2.212094279527664, 2.2730108308792114, 2.231453881561756]\n",
      "NLLLoss:\n",
      "[2.2589722231030462, 2.3939952838420866, 2.3388511389493942, 2.313135607242584, 2.136678504347801, 2.1386983948946, 2.2003092336654664, 2.1212553709745405, 2.149138662815094, 2.156963657736778, 2.1287244445085527, 2.1765494625270367, 2.194986729621887, 1.9816330878436565, 2.176300084888935, 1.9641265647113324, 1.9950997203588485, 1.9688079485297203, 2.054890798330307, 2.0253399986028673]\n"
     ]
    }
   ],
   "source": [
    "print('crossentropy:')\n",
    "print(all_losses1)\n",
    "print('NLLLoss:')\n",
    "print(all_losses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'对比发现使用NLLLoss作为损失函数得到的loss更小 效果更好'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''对比发现使用NLLLoss作为损失函数得到的loss更小 效果更好'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
