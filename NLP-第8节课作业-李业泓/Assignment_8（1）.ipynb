{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder 等同于Encoder-decoder，又叫做编码-解码模型，主要是为了解决sequence2sequence问题。所谓sequence2sequence问题就是泛指\n",
    "一些sequence到sequence的映射问题，输入的sequence可以是文字，语音，图像，视频，输出的sequence可以使文字，图像。以处理一句文字来生成另\n",
    "外一句话为例，来直观的理解和说明:\n",
    "    对于输入的句子X进行编码，编码方式可以采用CNN,RNN,LSTM,GRU等多种方式，生成得到一个context向量，用于储存语义信息\n",
    "    然后再通过解码器decoder，将生成的context和已经存在的历史信息y来生成新的输出信息\n",
    "    根据输入信息xi,yi通过encoder-decoder逐一生成y_hat\n",
    "    这样就达到了系统根据输入句子X生成输出句子Y的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search 通俗理解就是进行每一步搜索时，都只保留最优的也就是最大的输出概率，直到出现终止符或者句子达到最大长度\n",
    "beam search 是viterbi算法的贪心形式，beam search 通过设置beam size参数来限制在每一步中保存下来的可能性词的数量，\n",
    "greedy search可以当作是beam size 为 1 时的beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在众多信息中把注意力集中放在重要的点上，选出关键的信息，从而忽略其他不重要的信息，对于不同重要程度的输入信息赋予不同的权重，\n",
    "越重要对于输入影响越大的信息权重越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先计算query与k1,k2...ki的相似度:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Similarity(Query,Key_i) = Query * Key_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后进行softmax，将得到的相似度转换成和为1的概率分布:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a_i = Softmax(Sim_i) = \\frac{e^{Sim_i}}{\\sum_{j=1}^{L_x}e^{Sim_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Query,Source) = \\sum_{i=1}^{L_x}a_i * Value_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不能够处理一词多义的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo的本质思想:预先使用语言模型学习好一个单词的Word Embedding，根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的\n",
    "Word Embedding就能够根据不同的语境表达在这个上下文中的具体含义\n",
    "输入：预先训练好的word embedding\n",
    "中间：双层双向LSTM模型\n",
    "输出：输入层和双层LSTM每一层的输出，三个向量根据以下公式进行最后的加权求和得到一个向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ELMo_k^{task} = E(R_k;Θ^{task}) = γ^{task}\\sum_{j=0}^{L}S_j^{task}h_{kj}^{LM} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer能够并行计算，而RNN必须按照传递顺序一个一个计算\n",
    "Transformer长程联系很好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer normalizaition指的是在一个词语向量的序列上进行标准化，而batch normalization指的是在多个词语的向量的相同位置进行标准化，\n",
    "是没有意义地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的是为了给予输出的词位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention公式 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention指的是source=target情况下的注意力机制，前面介绍过注意力机制，self-attention也就是Q，K，V均来自同一个输入，经过不同\n",
    "的WQ，WK，WV，通过Attention公式得到输入X的新向量\n",
    "multi-head attention 指的是 不止一组Q，K，V，而是通过多个(WQ1，WK1，WV1),(WQ2,WK2,WV2)..(WQn,WKn,WVn)得到多组(Q1，K1，V1),\n",
    "(Q2,K2,V2)...(Qn,Kn,Vn)分别代入公式中，得到做足attention，最后在经过一词W转换成输入X的新向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于分类问题，给输入的句子加上起始符合终止符，然后通过transformer和一个全连接神经网络就能进行分类了<br>\n",
    "对于句子之间关系的判断，在两个句子中间加一个分隔符<br>\n",
    "对于文本相似性判断，把两个句子输入的顺序颠倒一下<br>\n",
    "对于多项选择问题，采用多路输入，每一路把文章和答案选项拼接作为输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 随机选择语料中15%的单词，用[Mask]掩码代替原始单词\n",
    "2. 然后要求模型去正确预测被抠掉的单词\n",
    "3. 15%的被选中要执行[mask]替身任务的单词中，只有80%真正被替换成[mask]标记\n",
    "4. 10%随机替换成另外一个单词，10%这个单词不做改动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.word embedding\n",
    "2.segment embedding\n",
    "3.postion embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.对于句子关系类任务，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer\n",
    "  最后一层位置上面串接一个softmax分类层即可。<br>\n",
    "2.对于分类问题，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造。<br>\n",
    "3.对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。<br>\n",
    "4.对于机器翻译或者文本摘要，聊天机器人这种生成式任务，引入Bert的预训练成果，在单个Transformer结构上加装隐层产生输出也是可以的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT和GPT2采用单向语言模型，GPT2与GPT的区别是，GPT2更深，使用预训练的数据量更大，而BERT采用双向语言模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
