{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is independent assumption in Naive bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯为什么朴素就是因为假设各个特征之间相互独立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML：极大似然估计是建立在极大似然原理上的一种参数估计得方法，通过若干次实验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率为最大。极大似然原理：是频率学派最经典的方法之一，认为真实发生的事件的结果的概率应该是最大地，相应地参数，也应该是能够使该状态发生的概率最大时的参数。极大似然估计假设各样本之间互相独立，可以写出似然函数$\\prod_{i=0}^nρi$，然后通过确定当似然函数取极大值时的$ρ_i$的取值，便能确定实验中某一现象发生的概率\n",
    "\n",
    "MAP：最大后验估计是贝叶斯学派的一种参数估计的方法，贝叶斯学派认为世界不是固定地，在进行估计之前，人们对于要估计的实物应该先有一个经验性的判断，然后根据数据调整对这个实物的判断。而这个经验性的判断就是先验概率，而经过调整之后的概率称作后验概率。最大后验估计的基础是贝叶斯公式：$P(\\theta|data)=\\frac{P(data|\\theta)\\times P(\\theta)}{P(data)}$,$P(\\theta)$就是先验概率，$P(data|\\theta)$是似然函数，$P(data)$就是data事件发生的概率，$P(\\theta|data)$后验概率表示的意思就是在已知事件data已经发生了的情况下$\\theta$的概率，既然data事件已经发生了，那么$P(data)$就为1，所以最大后验估计的公式就可以写成是$P(\\theta|data)=P(data|\\theta)\\times P(\\theta)$，因此我们可以看到最大后验估计就是在极大似然估计的基础上再增加一个对于事物的经验性判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is support vector in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量就是离超平面最近的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM是一种用于二分类的分类模型，其学习策略是间隔最大化，在样本数据特征空间中需要找到一个超平面，使得这个超平面离两边的样本数据间隔最大，这样就能确保分类具有更大的可信度。这个间隔被定义为几何间隔。在超平面WX+b=0已经确定的情况下，|WX+b|表示样本点到超平面的距离，我们可以通过观察WX+b与y的正负值是否一致来判断分类的准确性，那么|WX+b|就可以被改写成y*(WX+b)，这就是函数间隔。而几何间隔就是$\\frac{y*(WX+b)}{\\lVert W \\rVert}$,除以一个W的范数，为了防止W和b可以无限成倍的扩大与缩放。只需要求得max$\\frac{y*(WX+b)}{\\lVert W \\rVert}$便能寻找到最优的超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Shortly describ what 'random' means in random forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林就是随机产生n个决策树，然后让这n个决策树同时对待预测样本进行预计，最后选出结果占比最大的预测结果作为输出。\n",
    "随机主要体现在两个方面：<br>\n",
    "1.样本数随机。每次在构建一棵决策树的时候，并不是都使用所有的数据，而是在给定的训练数据中有放回的随机抽样，采样n次后把这个子数据集作为该决策树的根节点来构建一棵决策树<br>\n",
    "2.特征随机。每次也并不是运用全部的特征，而是从M个特征中随机选择出m个特征作为特征构建决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What criterion does XGBoost use to find the best split point in a tree ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 划分准则运用了贪心策略+最优化。在选择最佳划分属性的时候通过枚举法，逐一在每个特征属性中选择最佳的划分节点，判断最佳划分节点主要是通过计算出选择该节点划分后的loss function的增益，具体公式为$Gain=\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}]-\\gamma$，产生loss function下降最大的作为该属性的划分节点，同时作为该属性的分数，然后比较所有的属性哪个属性的loss function下降最大就选择哪个属性作为分裂属性，将树进行划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
