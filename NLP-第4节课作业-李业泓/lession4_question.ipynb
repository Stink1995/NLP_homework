{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron compute 是一种模仿人脑神经元的计算方式，通过输入层，隐藏层，输出层来构建计算图，输入层输入初始化数据，然后在隐藏层中得到各个输入\n",
    "# 节点之间的不同线性组合，再利用激活函数，再输出到下一层，以此类推，原则上隐藏层可以有无限多层，最后输出到输出层得到我们想要的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用线性激活函数，那么神经网络相当于只是把我们的输入线性组合然后再输出，中间的隐藏层便没有了意义，无论有多少隐藏层都相当于只是在做\n",
    "# 线性组合，唯一可以用到线性激活函数的地方就是输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Logistic Loss'主要用于二分类问题,由于最后要得到0-1形式的二分类数据，假设数据服从伯努利分布，那么输出标签为1的概率就可以设为hθ(x),那么输出为0的概率就是1-hθ(x)，根据伯努利原理:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(y=1|x;θ) = h_θ(x)  $$ \n",
    "$$ P(y=0|x;θ) = 1- h_θ(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将以上两个等式统一在一起:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(y|x;θ) = h_θ(x)^y*(1- h_θ(x))^{1-y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 然后写出P(y|x;θ)的似然函数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(θ) = P(\\vec{y}|X;θ)= \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};θ) $$\n",
    "$$ L(θ) = \\prod_{i=1}^{m}(h_θ(x^{(i)}))^{y^{(i)}}*(1-h_θ(x^{(i)}))^{1-y^{(i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 然后对似然函数取对数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ l(θ)=logL(θ)= \\sum_{i=1}^{m}y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l(θ)就是逻辑损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "A. ReLU\n",
    "B. Leaky ReLU\n",
    "C. sigmoid\n",
    "D. tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断图片中是否含有cats是一个二分类问题只需要输出0-1，因此可以选用C sigmoid激活函数\n",
    "# 除了二分类问题外，其他情况一般都会选用 A ReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果把初始参数设置成0，那么第一层的节点就都相等，具有对称性，在做反向传播的时候，相应偏导数也相等，这样的话设置多个隐藏单元就失去了意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Can you implement the softmax function using python ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ σ(z)_j = \\frac{e^{z_{j}}}  {  \\sum_{k=1}^K e^{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1, 1, 5, 3],\n",
    "     [0.2, 0.2, 0.5, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(data):\n",
    "    data -= np.max(data)\n",
    "    x_exp = np.exp(data)\n",
    "    x_exp_sum = np.sum(np.exp(data),axis=1,keepdims=True)\n",
    "    return x_exp / x_exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01562812, 0.01562812, 0.85326667, 0.11547709],\n",
       "       [0.23503441, 0.23503441, 0.31726326, 0.21266793]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
