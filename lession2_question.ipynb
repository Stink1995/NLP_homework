{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What conditions are required to make the BFS return the optimal solution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个节点之间的消耗cost的值不能为负数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Is there a way to make DFS find the optimal solution ? (You may need to read some material about iterative DFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制DFS的最大深度max_deep，在该深度范围内使用DFS搜索，如果在该深度范围内找到解则直接返回，若没有找到解，则max_deep+1,继续重复上述操作\n",
    "# 直到找到解为止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 In what conditions BFS is a better choice than DFS and vice versa ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFS与DFS相比时间复杂度相对较小但空间复杂度大，当需要寻找到最短路径时一般使用bfs，但当数据量非常大时，BFS需要存储之前访问过的所有数据，空间\n",
    "# 消耗比较大，这个时候使用DFS比较合适，但DFS在数据量大的时候就容易陷入无限树深的困境，可以综合BFS和DFS的思想使用IDDFS迭代深化深度优先搜索，\n",
    "# 通过限制树深的DFS来寻找最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 When can we use machine learning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当一些数据包含了一些信息，而这些信息通过人为的分析是很难找出规则地，这种情况就需要运用机器学习\n",
    "# 简单概括就是需要包含三个要素:\n",
    "# 1 存在一个模式或者说表现可以让我们对它进行改进提高\n",
    "# 2 通过人为规则并不是很容易定义\n",
    "# 3 需要有大量数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 What is the gradient of a function ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降就是通过迭代的方法，来找到loss function的局部最小值，迭代的方向是梯度下降的方向，为什么要选择梯度下降的方向是因为梯度是指当前点\n",
    "# 上升最快的方向，那么负梯度方向也就是下降最快的方向，方便我们更快的到达局部最小值。在loss function上随机取一点，求该点关于参数theta的偏\n",
    "# 导，也就是梯度的方向，让后将学习率rate和梯度相乘得到参数theta每次变化的量，然后用theta减去变化量，不断地对theta进行更新，直到到达局部最\n",
    "# 小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 How can we find the maximum value of a function using the information of gradient ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用梯度上升法 根据频率学派的思想，认为真实发生的事件的结果的概率应该是最大地，相应地参数，也应该是能够使该状态发生的概率最大时的参数，\n",
    "# 根据极大似然函数，我们就能使用梯度上升的算法不断地增加参数theta，当theta不再发生改变或者改变很微小时，我们认为theta已经达到最大，那\n",
    "# 么相应地也能求得目标函数的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
